{"meta":{"title":"Weikun Han's Website","subtitle":"","description":"May world peace, without pride and prejudice 愿世界和平，没有骄傲和偏见","author":"Weikunu Han","url":"https://weikunhan.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-08-19T02:45:17.176Z","updated":"2020-05-30T08:38:52.382Z","comments":true,"path":"404.html","permalink":"https://weikunhan.github.io/404.html","excerpt":"","text":"404 Sorry The address may be incorrect or may have been deleted"},{"title":"Courses","date":"2020-08-19T02:45:17.177Z","updated":"2020-06-03T01:10:45.020Z","comments":true,"path":"courses/index.html","permalink":"https://weikunhan.github.io/courses/index.html","excerpt":"","text":"Teaching Signals and Systems I, Iowa State University, Teacher Assistant, Fall 2014 Selected Courses Machine Learning, Stanford University, Coursera Introduction to Self-Driving Cars, University of Toronto, Coursera Convex Optimization, University of California Los Angeles, Spring 2018 Large Scale Social and Complex Networks: Design and Algorithms, University of California Los Angeles, Spring 2017 Large-Scale Data Mining: Models and Algorithms, University of California Los Angeles, Winter 2017 Favored Books Aurélien Géron, Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, O’Reilly Media, 2017 Ian J. Goodfellow and Yoshua Bengio and Aaron Courville, Deep Learning, MIT press, 2016 Bill Chambers, Matei Zaharia, Spark: The Definitive Guide, O’Reilly Media, 2018 Jiawei Han, Micheline Kamber, Jian Pei, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, 2012 Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford, Introduction to Algorithms, 2001"},{"title":"Publications","date":"2020-08-19T04:54:13.188Z","updated":"2020-08-19T04:54:13.188Z","comments":true,"path":"publications/index.html","permalink":"https://weikunhan.github.io/publications/index.html","excerpt":"","text":"Selected Journal Publications Tape nanolithography: a rapid and simple method for fabricating flexible, wearable nanophotonic devicesQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongMicrosystems and Nanengineering, 4, 31, 2018 See full publication list Selected Conference Publications Tape-based flexible metallic and dielectric nanophotonic devices and metamaterialsQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongIEEE 17th International Conference on Nanotechnology (IEEE-NANO), 2017 Best Student Paper Award See full publication list"},{"title":"Journal Publications","date":"2020-08-19T02:45:17.179Z","updated":"2020-05-30T08:38:52.391Z","comments":true,"path":"publications/journal_publications/index.html","permalink":"https://weikunhan.github.io/publications/journal_publications/index.html","excerpt":"","text":"Computer VisionMicroelectromechanical Systems2018 Tape nanolithography: a rapid and simple method for fabricating flexible, wearable nanophotonic devicesQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongMicrosystems and Nanengineering, 4, 31, 2018 2016 Strain-tunable plasmonic crystal using elevated nanodisks with polarization-dependent characteristicsYifei Wang, Longju Liu, Qiugu Wang, Weikun Han, Meng Lu and Liang DongApplied Physics Letters, 108, 071110, 2016 Electrically tunable quasi-3-D mushroom plasmonic crystalQiugu Wang, Weikun Han, Peng Liu, Liang DongJournal of Lightwave Technology, 34, 2175-2181 2016"},{"title":"Conference Publications","date":"2020-08-19T02:45:17.179Z","updated":"2020-05-30T08:38:52.389Z","comments":true,"path":"publications/conference_publications/index.html","permalink":"https://weikunhan.github.io/publications/conference_publications/index.html","excerpt":"","text":"Computer VisionMicroelectromechanical Systems2017 Tape-based flexible metallic and dielectric nanophotonic devices and metamaterialsQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongIEEE 17th International Conference on Nanotechnology (IEEE-NANO), 2017 Best Student Paper Award 2015 Strain-tunable two-dimensional plasmonic crystalsYifei Wang, Longju Liu, Qiugu Wang, Weikun Han, Meng Lu and Liang DongIEEE Photonics Conference (IPC), 2015"},{"title":"","date":"2020-08-19T02:45:17.177Z","updated":"2020-05-30T08:38:52.386Z","comments":true,"path":"blogs/index.html","permalink":"https://weikunhan.github.io/blogs/index.html","excerpt":"","text":""},{"title":"Projects","date":"2020-09-12T07:19:43.639Z","updated":"2020-09-12T07:19:43.639Z","comments":true,"path":"projects/index.html","permalink":"https://weikunhan.github.io/projects/index.html","excerpt":"","text":"Projects Research Projects0pen Source ProjectsIndividual Projects Superpixels Generator Coming soon Coming soon Tripadvisor Web Crawler LeetCode Python LeetCode C++ LeetCode Java Comeing soon"},{"title":"Living in the high dimensional space","date":"2020-09-23T07:00:00.000Z","updated":"2020-09-23T21:48:26.562Z","comments":true,"path":"blogs/ideas_sharing/living_in_the_high_dimensional _space/index.html","permalink":"https://weikunhan.github.io/blogs/ideas_sharing/living_in_the_high_dimensional%20_space/index.html","excerpt":"","text":"I had some crazy ideas recently after I took Andrew Ng’s course. I consider that maybe the universe is like deep neural networks, which is not organized by low dimensional space and it is organized by high dimensional space. It is true that humans do not easily observe the high dimensional space. The universe is like a deep neural network’s high dimensional space is not observed by humans’ intuition but not means it does not exist. 我上了吴安德的课程后，最近有一些疯狂的想法。我认为宇宙也许就像是深度神经网络，不是由低维空间组织的，而是由高维空间组织的。 的确，人类不容易观察高维空间。 宇宙就像一个深层神经网络的高维空间，不是人类的直觉所观察到的，但这并不意味着它不存在。 In this case, the whole universe is like a non-convex function, and there are many local optimization points. From mathematics, we know that the derivate of local optimization points is 0. In other words, if some matters exist in those local optimization points, the gradient (rate of change) is 0. Thus, those matters in the local optimization point will not change, but some matters are not in the local optimization point will change a lot. If this idea is right, many scientists can observe some things in some local optimization points. It may easily dig more secrets from the universe. 在这种情况下，整个宇宙就像一个非凸函数并且有许多局部优化点。 从数学上，我们知道局部优化点的导数为0。换句话说，如果局部优化点中存在某些物质，则梯度（变化率）为0。因此，局部优化点中的那些物质不会变化，但是有些事情不在本地优化点上会改变很多。 如果这个想法正确，那么许多科学家可以在某些局部优化点中观察到一些东西。 它很容易从宇宙中挖掘更多的秘密。 The above is a crazy idea, you can treat it like a joke if you like. 以上是一个疯狂的主意，如果您愿意，您可以像个玩笑一样对待。"},{"title":"Awards","date":"2020-08-19T02:45:17.176Z","updated":"2020-05-30T08:38:52.385Z","comments":true,"path":"awards/index.html","permalink":"https://weikunhan.github.io/awards/index.html","excerpt":"","text":"Best Paper Award Best Student Paper Award, IEEE NANO, 2017, Tape-based flexible metallic and dielectric nanophotonic devices and metamaterials Others Magna Cum Laude Honor, Iowa State University, 2015, Ranked 1st in a class of 2015 Electrical Engineering graduates (Winter) Dean’s List Honor, Iowa State University, 2013 - 2015, Given to top ranked students"},{"title":"Wrote before deciding to proceed my Ph.D.study","date":"2020-04-29T07:00:00.000Z","updated":"2020-07-25T19:22:53.315Z","comments":true,"path":"blogs/stories_recording/wrote_before_deciding_to_proceed_my_phd_study/index.html","permalink":"https://weikunhan.github.io/blogs/stories_recording/wrote_before_deciding_to_proceed_my_phd_study/index.html","excerpt":"","text":"Due to COVID-19, I have time to stop and think about what I should pursue in the next five years. I am lucky that can join one startup company (Clobotics Global), which force on use computer vision to provide end-end AI solution for global consumer goods companies. During two years of working in Clobotics Global, I was very honored to collaborate with excellent engineers and scientists to solve many real-life challenging problems. I had lots of great chances that could use what I learn from before to implement many ideas. I was lucky that I could be mentored by many excellent industry leaders. Here, I want to thank Dr. Yipeng Li, Dr. Feng Zhu, Dr. Albert Chen, and Dr. Yan Ke. At the same time, I feel there are still have a long way to go to let more and more AI-related products to walk into people’s life. For example, 1)related infrastructure development for AI-related products, 2)computation cost for model serving/training, 3)labeling cost for a supervised learning task, 4)uncertainty in the deep learning-based model for challenging computer vision tasks, 5)and more advanced and challenging real-life computer vision tasks. In the next decade, I believe people’s lifestyles will become more and more intelligent. Lots of great companies like Intel, Nvidia, and AMD are putting lots of sources on the research to provide more powerful and high-performance computing hardware. Google, Facebook, Amazon, and Microsoft are putting lots of sources on the research to provide more high-efficiency, convenient, and robust service. However, It is not enough and only a beginning, there are lots of problems that need a more effective way to solve it. Therefore, that is the reason why I want to back to the academy, dive deep, and generate a more great idea to solve those real-life challenging problems. I plan to conduct research in the following reaches area: Algorithm optimization in computer vision: Could we provide a more robust model in many other tasks such as Fast R-CNN in object detection? For example, there are many challenging subtasks in five main computer vision tasks (image classification, object detection, target tracking, semantic segmentation, instance segmentation) that need a more robust model to provide better performance. Learning method in computer vision: How could let machine study like people? Currently, people still use supervised learning as main training method for five main computer vision tasks in the industry. This training method requires tons of labeled data which cause extremely high cost for company operation. Is there exist a better way to do it? For example, “self-supervised learning” that would demonstrate more powerful compare with bringing humans into the loop for facilitating bias discovery in image datasets Advanced data in computer vision: Some challenging computer vision tasks may not solve by current popular technology. For example, base on 2D image data, how could let the machine confidently identify 16.9 fl oz Coca-Cola Soda with 12 fl oz Coca-Cola Soda from different perspectives? Therefore, I believe that we could involve 3D image data to solve those tricky problems. 2020年5⽉29号是我在公司的最后⼀天。由于疫情的原因，先后遇到⼯作、⽣活、健康⽅⾯的各种困难。正因为如此，我有时间停下来思考，未来的五年我应该追求的⽬标。 我有幸加⼊⼀家初创公司（扩博智能—借助计算机视觉为全球消费品公司提供端到端的AI解决⽅案）。在扩博智能⼯作期间，我很荣幸与优秀的⼯程师和科学家合作，利⽤计算机视觉技术解决很多现实⽣活中的难题，让生活更加智能。期间，我还有大量的机会可以利⽤以前学到的东⻄来实现许多自己想法。同时，我很幸运能得到许多杰出⾏业领袖的指导。 在这⾥，我要感谢Yipeng Li博⼠、Feng Zhu博⼠、Albert Chen博⼠和Yan Ke博⼠。 在近两年的⼯作实践中，我认为要让越来越多的AI相关产品进⼊⼈们的⽣活还有很⻓的路要⾛。 诸如：⼈⼯智能相关产品的基础设施开发；如何降低模型服务以及训练的成本；如何降低监督学习任务的数据标记成本；如何解决基于深度学习的计算机视觉模型的不确定性；以及解决更具挑战性的计算机视觉任务。 在未来的⼗年中，我相信⼈们的⽣活⽅式将变得更加智能。 许多伟⼤的公司，例如英特尔、英伟达和AMD，都在科研中投⼊了⼤量资源来开发功能更强⼤和性能更⾼的计算硬件。 ⾕歌，脸书和微软也都在研究中投⼊了⾮常多资源来提供更⾼效、更便捷、更加智能的服务。但是，这还远远不够，⽽且仅仅是⼀个开始，有许多问题需要更有效的⽅法来解决。 因此，我决定带着这些问题，开始为期5年的博⼠研究⼯作。希望在此期间，我可以对固有的基础知识有更扎实的理解、对AI相关领域有更深的看法、以及对越来越多的AI相关产品⾛⼊⽣活提供更好的解决⽅法。 最后，再次感谢公司近两年的栽培，也祝福⾃⼰在未来可以有更多的突破。 Contact Me GitHub LinkedIn Google Scholar Curriculum Vitae"},{"title":"CVPR 2020 | Using SpixelFCN to Generate the Distinctive Superpixels","date":"2020-09-27T18:25:16.559Z","updated":"2020-09-27T18:25:16.559Z","comments":true,"path":"blogs/papers_reading/using_spixelfcn_to_generate_the_distinctive_superpixels/index.html","permalink":"https://weikunhan.github.io/blogs/papers_reading/using_spixelfcn_to_generate_the_distinctive_superpixels/index.html","excerpt":"","text":"1 Overview 综述In computer vision, superpixels help to reduce the number of image primitives for subsequent processing. It converts the pixel-level image to the district-level image, which can be treated as the abstraction of original image information. There are many methods to generate superpixels but only have few attempts to generate it using deep neural networks. Recent paper Superpixel Segmentation with Fully Convolutional Networks, the researchers from The Pennsylvania State University proposed using a fully convolutional network to generate superpixels. The paper was published on CVPR2020, and superpixels would become much more popular with this state-of-art method. 在计算机视觉中，超像素有助于减少用于后续处理的图像基元的数量。 它将像素级别的图像转换为区域级别的图像，可以将其视为原始图像信息的抽象。 生成超像素的方法有很多，但是只有很少的尝试使深度神经网络生成超像素。最近一篇论文《Superpixel Segmentation with Fully Convolutional Networks》，宾大州立的研究人员提出了使用了全卷积网络来生成超像素的方式，论文发表在CVPR2020中。这种全新的超像素生成方式，给其应用带来更广阔的空间。 In the paper, the author has pointed out the challenges to use a fully convolutional network to generate superpixels: “The standard convolution operation is defined on regular grids and becomes inefficient when applied to superpixels.” Therefore, the author used a novel method that accelerates the superpixels generation. 作者在文中指出，使用全卷积生成超像素的挑战在于：“标准卷积运算是在规则网格上定义的，当应用于超像素时其效率会变的很低”。所以本文作者运用了一种更高效的方式，可以使用全卷积网络来快速生成超像素。 1.1 Summary 概要In this paper: The author first proposed a method to generate superpixels with a fully convolutional network. According to experimental results, their method is comparable to state-of-the-art superpixel segmentation performance. At the same time, the superpixels generation speed is 50fps. The author second developed an architecture for dense prediction tasks based on predicted superpixels, which can boost the performance to generate high-resolution outputs. The architecture is combined with a fully convolutional network (used for generated superpixels) to into popular network architecture for stereo matching. In this way, it helps improve disparity estimation accuracy. 在这篇论文： 作者首先提出了一种用全卷积网络生成超像素的方式。通过结果来看，本方法可以和当今最流行的方法媲美，同时生成超像素的速度可以达到50fps。 其次，作者提出可以将生成的超像素用于后续的密度预测任务，并且有助于得到更好的结果。其思路是，将用于生成超像素的全卷积网络与主流的立体匹配网络相结合，帮助立体匹配网络产生更准确的差距。 1.2 Advantages 优势In my opinion, there are two points I can learn from it: First, using a fully convolutional network to fast generate superpixels by solving primary inefficient issues. In the future, I can try to use different deep neural networks to generate superpixels. For example, diverse architectures or operators. In this paper, the fully convolutional network is a simple encoder-decoder architecture. I am surprised that why so many researchers could not think of such a simple idea. Most of the time, make complex things more complect may not be a good choice. I should consider the fundamental motivations behind things. For example, in this paper, the author pointed out: “Superpixel is inherently an over-segmentation method. As one of the main purposes of our superpixel method is to perform the detail-preserved downsampling/upsampling to assist the downstream network, it is more important to capture spatial coherence in the local region.” Based on this motivation, the author first inspired an initialization strategy from traditional superpixel algorithms. Then they used a fixed regular grid to find out the local region information rather than compute all pixel-superpixel paris. Finally, using the advantages of a fully convolutional network, superpixel assigned as the highest probability of region pixels. In the end, based on this design idea, it successfully solves inefficient when using deep neural networks to generate superpixels. Second, the fully convolutional network used to generate superpixels can be more easily combined with many deep neural networks to improve the the overall performance in different computer vision tasks. For example, the author only tried to use superpixels in the dense prediction tasks, which let superpixels assist improve stereo matching performance. Therefore, could this idea be applied to different computer vision tasks to improve related deep neural network performance? 个人认为，本文有两个地方可以借鉴： 第一，通过解决主要的低效率问题，使全卷积网络可以快速生成超像素。未来可以尝试用更多不同的深度神经网络去生成超像素。比如，不同的结构或者不同的算子。在本文中的全卷积网络是一个编码器-解码器架构。我很好奇，这么通用并且简答的架构，为什么很多研究人员想不到？很多时候，将复杂的结构变的更复杂，可能不如先想明白事情的本质。比如说，作者的在文中指出超像素的本质是：“一种过度分割的方法。其目的是：“为了保留下采样/上采的细节，这样可以辅助下游网络去更好的捕获在某些区域的空间连贯性。”基于这样的目的，作者首先借鉴初始化步骤在传统的超像素生成算法中。其次，利用局部而非全局信息。最后，结合全卷积网络的优势找出局部信息中最有可能点作为超像素的点。这样的设计理念，成功的解决的了用深度神经网络生成超像素的低效问题。 第二， 用于生成超像素的全卷积网络，可以更容易的和很多深度神经网络结合，去提升深度神经网络在不同计算机视觉任务中的性能。例如，作者在文本中只在密度预测任务中，尝试用超像素去辅助主流的立体匹配网络提升其性能。所以，是否可以将这样的想法运用到不同计算机视觉任务中，去提升相应的深度神经网络的性能？ 1.3 Disadvantages 劣势In my opinion, two points need to pay attention: First, by using a fully convolutional network to generate superpixels, the author only proposed to take advantage of 3 x 3 regular grid (total 9 grid cells included). In other words, such local information only produces the fixed number of superpixels. If there are some cases or data, it is hard to get more superpixels and to control the number of the generated superpixel. Second, when the fully convolutional network used to generate superpixels combined with deep neural networks, it could reduce the output efficiency even though it may make results better. In this paper, the author stated that the superpixels generation speed is 50fps. Tt possible that the overall inference speed is about 50fps or even lower after combining all together. Therefore, the idea may be hard to apply to many applications if there is no better way to solve these problems. 个人认为，本文有几个地方需要注意： 第一，在作者提出利用全卷积网络生成超像素的方式中，所用的局部信息只有一个3 x 3 区域（也就是9个网格包域）。也就是说，这样局部信息只能生成固定的超像素。如果遇到特定的情况或者特定的数据，很难去生成更多的超像素，同时也很难去控制生成指定数量的超像素。 第二， 将生成超像素的全卷积网络和主流深度神经网络结合，在提升性能的同时，可能会降低其输出的效率。本文中，作者指出生成超像素的速度可以达到50fps。也就说，如果将全卷积网络生成超像素和主流深度神经网络结合，那么其输出结果的效率大概率会被锁定在50fps左右或者更低。因此，如果找不到一种办法可以加速全卷积网络生成超像素，那么可能很难应用到实际中。 1.4 Future 后续The author will try a similar idea: let the fully convolutional network used to generate superpixels combined with other deep neural networks. It may help with many computer vision subtasks: target segmentation and optical flow estimation tasks. Also, the author will explore more applications that can introduce superpixels. 作者在提出，会将生成超像素的全卷积网络和不同主流深度神经网络结合， 去尝试处理不同的计算机视觉子任务：目标分割和光流估计任务。并且，作者会探索不同的方式去让超像素得到更广的应用。 In my opinion, the first point raised by the author is the same as what I thought before (1.2 Advantages, second point). Also, I believe that when I find different superpixels application scenarios, I can try to use different deep neural networks to generate superpixels. For example, For example, diverse architectures or operators. (1.2 Advantages, first point). 个人认为，作者提出的第一点和我在之前想的（1.2 优势，第二点）一样。除此以外，我觉得当找到不同的超像素应用场景的时候，可以尝试用更多不同的深度神经网络去生成超像素。比如，不同的结构或者不同的算子（1.2 优势，第一点）。 2 Highlight Details 重点细节2.1 SpixelFCNThe first highlight of this paper is to propose a fully convolutional network to generate superpixels, SpixelFCN. Different from the traditional way of generating superpixels, the fully convolutional network only uses the local information to generate superpixels. The purpose of this is to reduce unnecessary computation. As shown in the figure below, there are many pixels in the green box and only collect the information in the 3 x 3 red boxes area. Finally, by learning a soft association map, the pixels with the highest probability in the 9 grid cells as superpixels. 本篇论文的第一个亮点是提出了一个全卷积网络生成超像素SpixelFCN。与传统的超像素的生成方式不同，利用全卷积网络生成超像素仅仅关注局部信息。这样做的目的是为了减少不必要的计算。如下图所示，绿框中有很多像素点只收索附近3 x 3红框区域内的信息。最终在9个网格包域中，通过学习到到的一个软关联图，将域中最高概率的像素点设置为超像素点。 Here, the soft association map can be learned as a fully convolutional network with a simple encoder-decoder architecture. As shown in the figure below, a schematic diagram and please refer to the supplementary materials of the paper for information. 这里的软关联图可以通过一个拥有编码器-解码器架构的全卷积网络学习获得。如下图所示，一个简答的示意图，具体的结构参考原论文的补充材料。 The author also compared with “Superpixel Sampling Networks” (SSN) in ECCV2018. As shown in the figure below, although they both introduced the fully convolutional network into the design, the author in this paper pointed out their architecture has fundamentally different than SSN. The former just uses the fully convolutional network as a tool, and finally uses K-Means to obtain global information to generate superpixels. The method in this paper generates superpixels with only a simple fully convolutional network without other complicated computations. 作者同时还比较了在ECCV2018中的”Superpixel Sampling Networks”（SSN）。如下图所示，虽然都是引入全卷积网络的想法，但是作者提出的结构和SSN有本质的不用。前者只是把全卷积网络用做一个工具，最后还是要通过K-Means来获取全局信息生成超像素。而后者，仅仅用一个简单的全卷积网络就可以搞定了，不需要其他复杂的计算。 In the paper, a generated superpixel is composed of two vectors. One represents the superpixel attributes, and the other represents the position of the superpixel. As in the following formula: 论文中，生成的超像素有两个向量组成，一个是代表超像素额属性，另一个代表超像素的位置。如下面公式： The pixels can be reconstructed through the above two vectors. As is the following formula: 通过这向量可以重建像素点，如如下面公式： Finally, the original pixels and reconstructed pixels can be written as a loss function. As in the following formula, the first term represents similarity, and the second term represents compactness of space: 最后可以将原始像素点和重建像素点，写成一个损失函数。如下面公式，第一项代表相似性，第二项代表空间的紧凑性： In addition, the author also tried to use the CIELAB color vector and L2 norm to write a new loss function. At the same time, the author pointed out that this is very similar to Simple Linear Iterative Clustering (SLIC). As is the following formula: 除此以外，作者还尝试用CIELAB色彩向量和L2模去计算。同时，作者指出这么做和Simple Linear Iterative Clustering（SLIC）就非常相似了。如下面公式: Keep simplifying the above loss function, which can be done with the one-hot encoding vector of semantic labels. As is the following formula: 继续优化上面的损失函数，可以用一键编码向量来做。如下面公式: The author did not stop at this point but tried to continue to optimize the network structure. Since the full convolutional network couldn’t learn the affinity of pixels in the image, the author introduced the Spatial Propagation (SP) proposed by “Learning Affinity via Spatial Propagation Networks” in NIPS2017. Because of building affinity matrix to propagate information in the full convolutional network, the author called Convolutional Spatial Propagation (CSP). As is the following formula: 作者并没有点到为止，而是尝试继续优化网络结构。由于普通的全卷积网络不具备学习图像中像素的相连性，作者这时候引入在NIPS2017中的“Learning Affinity via Spatial Propagation Networks”。其中的Spatial Propagation（SP）可以使全卷积网络可以学习图片像素点之间的相连性。作者这里叫做Convolutional Spatial Propagation（CSP）。如下面公式： A slight adjustment is needed above the equation. It should only compare the information of the nearby 3 x 3 red boxes as mentioned earlier. As is the following formula: 这里需要稍作调整，应为之前提到只对比附近3 x 3红框区域信息。所以，如下面公式： 2.2 SpixelFCN + PSMNetThe second highlight of this paper is to use the generated superpixels for subsequent density prediction tasks, which can help get better results. Especially, the paper mainly focuses on the stereo matching. In the stereo matching network, there are four channels: height, width, disparity, and feature. To aggregate that information, 3D convolution is required. And, the overall computation consumes large amounts of memory because of the extra “disparity” dimension. In order to generate high-definition results, the general solution is to do a regression analysis of disparity. However, such processing will result in blurring of the object boundary and loss of many details. 本篇论文第二个亮点就是将生成的超像素用于后续的密度预测任务，并且有助于得到更好的结果。作者在论文中选择的是具体应用场景是立体匹配。在立体匹配网络中，有四个通道：高，宽，视差，和特征。由于这样的特性，处理这样的信息需要3D卷积来帮忙。在其中，计算视差消耗了大量的的存储空间，导致无法生成高清的结果。为了解决这样的问题，通用额解决思路是做一个视差的回归分析。但是，这样的处理会导致目标边界的模糊和细节的丢失。 The author proposed to use superpixels as intermediate information to ensure that the regression analysis of disparity will not lose too much information. Therefore, the author used the “Pyramid Stereo Matching Network” (PSMNet) in CVPR2018 as the primary architecture and combined it with SpixelFCN. The author called it SpixelFCN + PSMNet. As shown below. 所以，作者提出了用超像素作为一个中间信息，去保证视差的回归分析不会导致目标边界的模糊和细节的丢失。作者用了CVPR2018中的“Pyramid Stereo Matching Network”（PSMNet）作为主架构，将生成超像素的全卷积网络与之结合。这个网络就是SpixelFCN + PSMNet。如下图所示。 The overall architecture is not changed too much. It add a downsampling/upsampling scheme based on the predicted superpixels and to integrate it into existing PSMNet. Therefore, the final loss function combines the sum of the losses of the first two networks. As is the following formula: 整个的架构其实没有太大的改动，就是将上采样网络和下采样网络分开加入到PSMNet中。所以，最后的损失函数结合前两个网络的损失和。如下面公式： 3 Experimental results 实验结果3.1 SpixelFCNAs shown in the figure below, the experiment results for superpixel generated by different algorithms on the BSDS500 dataset. The author provided three evaluation metrics. SpixelFCN and other methods are similar. 如下图，根据不同的算法生成的超像素实验结果在BSDS500数据集上。作者提供了三个测试的标准，SpixelFCN和其他方法对比都差不多。 As shown in the figure below, the experiment results for superpixel generated by different algorithms on the NYUv2 dataset. The author provided three evaluation metrics. SpixelFCN gets slightly better in the second item. 如下图，根据不同的算法生成的超像素实验结果在NYUv2数据集上。还是之前三个测试的标准，SpixelFCN第二项上有小幅的提升。 As shown in the figure below, compared to the previous two types of fully convolutional networks, using SpixelFCN to generate superpixels is very fast. The inefficient issue is the primary problem that this paper wants to solve. 如下图，相比之前两种以全卷积网络的来说，SpixelFCN生成超像素的速度非常快，表现很突出。这也是本论文主要想解决的问题。 As shown in the figure below, by using SpixelFCN to generate superpixels, it indicates that the object boundary becomes cleared and help preserve many details 如下图，超像素在实际图片中的结果，结果显示可以很好的处理目标边界的模糊和细节的丢失的问题。 3.2 SpixelFCN + PSMNetAs shown in the table below, the author listed the performance of SpixelFCN + PSMNet on SceneFlow and HR-VS dataset. SpixelFCN + PSMNet can provide fewer errors. 如下表，作者列出SpixelFCN + PSMNet在SceneFlow和HR-VS的数据集上的表现。SpixelFCN + PSMNet可以提供更少的误差。 As shown in the table below, the author listed the performance of SpixelFCN + PSMNet on Middlebury-v3 benchmark. SpixelFCN + PSMNet can provide a better result. 如下表，作者列出SpixelFCN + PSMNet在Middlebury-v3 benchmark上的表现。同样，SpixelFCN + PSMNet有更出色的表现。 As shown in the figure below, it shows the results of SpixelFCN + PSMNet. The results show that many tiny details can be well captured. 如下图，最后看看具体超像经过SpixelFCN + PSMNet的结果，结果显示可以很好的捕捉到具体的细节。 4 Note 最后寄语Welcome to communicate more with my friends who also like computer vision. 希望和喜欢计算机视觉的朋友多多交流。 5 Reference 引用 View All 查看全部 Fengting Yang, Qian Sun, Hailin Jin, Zihan Zhou, Superpixel segmentation with fully convolutional networks, CVPR, 2020"},{"title":"Standing on the Shoulders of Pytorch to Study Deep Learning","date":"2020-09-27T18:25:20.315Z","updated":"2020-09-27T18:25:20.315Z","comments":true,"path":"blogs/technologies_learning/standing_on_the_shoulders_of_pytorch_to_study_deep_learning/index.html","permalink":"https://weikunhan.github.io/blogs/technologies_learning/standing_on_the_shoulders_of_pytorch_to_study_deep_learning/index.html","excerpt":"","text":"1 Introduction 开门不见山With the development of deep learning, many deep learning frameworks are emerging. Now, the most popular deep learning framework should be PyTorch. The data show that in the 2020 CVPR, PyTorch related papers have four times as much as TensorFlow. After retiring from the company, I finally had an opportunity to try PyTorch. After starting using PyTorch, I just want to say: “I feel so good!” 随着深度学习发展，深度学习的框架也层出不穷。现在最流行的深度学习框架应该非PyTorch莫属了。有数据显示，在2020的CVPR论文中PyTorch占比是TensorFlow 4 倍。从公司退来之后，终于有机会接触PyTorch。上手PyTorch之后，只能用两个子形容：真香。 From the upsetting Caffe to the fast and stable TensorFlow, I treated PyTorch with rose-colored glasses in the past two years of work. To be honest, the previous PyTorch would be more suitable for research, and it didn’t very support the end-to-end model pipeline in the industry (data processing, model training, model testing, and model serving). The current PyTorch is a real leader in the deep learning framework market. First of all, to understand and customize classes in Pytorch is very easy due to it provides various capsulated classes, you only need inheritance the class that you want. Secondly, all APIs in Pytorch are user-friendly, full-featured, and convenient to use. Moreover, the official documentation is very detailed. There also have many official tutorials. Finally, the PyTorch community helps answer various questions. 在过去工作的两年里，从令人跺脚的Caffe到后来快速稳定的TensorFlow，我一直对PyTorch带着有色眼镜。讲道理，之前的PyTorch还是更适合做研究，对于工业界的一整套东西（数据整合，模型训练，模型验证，模型服务）支持的并不是很理想。现在的PyTorch，可谓是深度学习框架的翘楚。首先，各种类的封装做的没话说，理解和自定义类非常简单。其次，API设计的非常人性，功能很全，使用起来非常方便。最后，官方文档写的很详细，官方教程很多，并且还有PyTorch社区帮忙回答各种问题。 The interface of PyTorch is Python, but PyTorch mainly uses C++ to do the implementation. PyTorch uses a paradigm called the imperative style - eager execution. That is to say, each line of code requires the construction of a graph to define a part of the whole computation graph. Even if the overall computation graph has not yet finished, we can also independently execute these small computation graphs as components. This kind of method for a dynamic computation graph is called define-by-run method -Synced TechPyTorch的接口是Python，但底层主要都是用C++实现的。PyTorch使用一种称之为 imperative/eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为define-by-run方法。- 机器之心 SyncedTech The above PyTorch introduction comes from Synced Tech. This introduction of PyTorch maybe most people don’t know what is talking about, and it is not helpful for students to get started with PyTorch. However, the basic idea for this information could tell students that PyTorch has become fresh and refined because of this adventure design ideas. Since many students mainly concerned about how to get started PyTorch quickly, I will not introduce these design ideas here. 上面这段摘自机器之心对PyTorch基本描述看起来很高大上，其实对同学入门PyTorch并没有太大帮助。但是因为上面这个设计理念，让PyTorch变的清新脱俗。因为大部分入门PyTorch的同学主要关心如何快速上手，所以，这里我就不再对这些高大上的设计理念展开更多的介绍。 Isaac Newton said: “If I have seen further, it is by standing on the shoulders of giants.” Similarly, PyTorch became so popular because it stands on the shoulders of giants as well. Many people say: “PyTorch’s workflow is very close to NumPy (scientific computing library in Python).” Therefore, PyTorch is most likely to stand on the shoulders of NumPy. Later, I will explain why PyTorch is by standing on the shoulders of NumPy. 牛顿说过：“如果我看得更远一点的话，是因为我站在巨人的肩膀上”。 PyTorch之所以能变的如此流行，也是因为站在了巨人的肩上。PyTorch不是闭门造车，更不是从头一点一点开始。很多人说：“PyTorch的工作流程非常接近于Python的科学计算库NumPy”。如此说来，PyTorch站在了NumPy的肩膀上。后面的内容，我会说明为什么PyTorch站在了NumPy的肩膀上。 As there are many kinds of PyTorch tutorials, I listed some PyTorch learning resources at the end. I hope that students will get what they need for these learning resources. In this blog, I will introduce a way to get started with PyTorch quickly. By studying this learning method as an example, it is easy to get started with many large open-source projects. In the end, I hope this learning method can help many students. 由于PyTorch各类教程很多，我在最后列出了一些PyTorch学习资源，希望同学们对这些学习资源各取所需。这篇博客，我以介绍如何快速上手PyTorch为例子，同时介绍一种快速上手大型项目的方法。希望这样的学习方法可以帮助到一些学生。 2 拨开迷雾见月明Now, if you are very interested in PyTorch, let me take you into the world of PyTorch. Here, how to quickly get started with PyTorch and use it for actual projects? 现在，如果你对PyTorch非常感兴趣，让我带你来走进PyTorch世界。那么，如何快速上手PyTorch，应对实战项目呢？ Read official documentation 看官方文档 Read the open-source PyTorch project 看开源PyTorch项目 Read recommended PyTorch books 看推荐PyTorch书籍 Take PyTorch related class 上精品PyTorch网课 Code with PyTorch 大量练习写PyTorch代码！ From the above 5 points, practicing to write the PyTorch code is the most important thing! Generally speaking, the first step to learn a new tech is to check the open-source project. Next, you can try to read the official document. Now, let’s follow an example to shows how to learn a new tech by yourself. 上面5点，最重要的是大量练习写PyTorch代码！一般来讲接触到一个陌生的东西，第一步是源自开源项目，然后开始看官方文档。那么现在就看一个例子 - 如何正确的自学成才。 12import torchimport torch.nn as nn You will see many open-source PyTorch projects import above APIs. And, you can guess those two APIs are the most important in PyTorch. The first API is torch and it comes from NumPy. It not only has basic NumPy operation but also includes many basic functions that a deep learning framework should have. Because of torch API, it establishes a very popular deep learning framework - Pytorch. I can use the following example to illustrate why torch is the most essential API. Now, let us first try how to use torch API to achieve the function of torch.nn. 很多开源PyTorch项目会看到上面的API。这也是PyTorch最重要的两个APIs。第一个API是torch， 基本是NumPy换了一个马甲，但是同时增加很多一个深度学习的框架应该有的基本功能。可以说，正式因为torch，才组建了如此高大上的深度学习的框架。我可以用下面的例子来说明，让我们试试如何用torch实现torch.nn的功能。 123456789101112import torchimport torch.nn as nnx = torch.tensor([-4.5, 0.7, 3.3])y = torch.tensor([1., 0., 1.])h = nn.Sigmoid()criterion = nn.BCELoss()torch_nn_loss = criterion(h(x), y)print(&#x27;Logistic Regression Cross Entropy Loss use torch.nn: loss = &#123;&#125;&#x27;.format(torch_nn_loss.item()))torch_loss = -(y * torch.log(torch.sigmoid(x)) + (1 - y) * torch.log(1 - torch.sigmoid(x))).mean()print(&#x27;Logistic Regression Cross Entropy Loss use torch: loss = &#123;&#125;&#x27;.format(torch_loss.item())) The above code is to show a loss function for classification problem - logistic regression. The mathematical equation is as follows: 上面的代码展示了一个分类问题损失函数 - 逻辑回归。其数学表达式如下： $$\\begin{equation}J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\lbrack y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\rbrack\\end{equation}$$ The final result is the same from both implementations which are 1.884. From here, it is easy to see that using the torch operation can finial achieve many functions in torch.nn. The implementation may surprise some students and make it confidence for them to build their own deep learning frameworks. However, if directly use the torch operations to achieve many functions, it may need to write tens of thousands of extra lines of code. Not only that, but also this kind of code is less easy to maintain, hard to scale up, not possible to optimation, and became a potential risk for the compatibility. Therefore, PyTorch developers use a very important idea to design it - Object-Oriented Design. In the end, follow many design ideas and NumPy. They developed torch.nn, and following APIs: 最后结果两边的计算结果都是1.884。从这里开始很明显的看出：用torch基本的算子可以实现torch.nn的功能。看到这里，是不是感到很惊讶，感觉自己可以去做一个深度学习的框架了？别急，如果直接用torch的基本的算子实现的各种函数功能，可能需要写上万行多余的代码。而且这样的代码，对后期的维护、功能扩展、代码的性能优化、和其他硬软件的兼容都是灾难的铺垫！所以，PyTorch的开发人员利用编程中很重要的基本思想 - 面向对象编程。这个可能你都不清楚的基本思想，加上套了马甲的NumPy。不仅实现了torch.nn，还实现了： View All 查看全部 torch.nn.functionaltorch.TensorTensor AttributesTensor Viewstorch.autogradtorch.cudatorch.cuda.amptorch.distributedtorch.distributionstorch.hubtorch.jittorch.nn.inittorch.onnxtorch.optimQuantizationDistributed RPC Frameworktorch.randomtorch.sparsetorch.Storagetorch.utils.bottlenecktorch.utils.checkpointtorch.utils.cpp_extensiontorch.utils.datatorch.utils.dlpacktorch.utils.model_zootorch.utils.tensorboardType InfoNamed TensorsNamed Tensors operator coverageAnd：torchaudiotorchtexttorchvisionTorchElasticTorchServe All APIs above exist in Pytorch 1.5. Later on, if there are subsequent versions for PyTorch, please reference the official document! Some students may not feel good once reading such large and complex documents. However, as an example, I previously listed. Those APIs all inherit from the basic operations - torch. Therefore, you should be able to take easy for such large and complex APIs and understand a study method: 上面是列出的所有1.5版本所有PyTorch的API。日后如果PyTorch继续发达了，请参考官方文档！一般同学看到这么复杂的文档，心情肯定是很糟糕的。但是，正如我最开始举的例子，它们都是继承者们，没错是torch的继承者们。所以，读到这里，你应该明白一件事情： The best way to quickly get started with large open-source projects is to understand the core component of the project. 想要快速上手大型开源项目，首先抓住这个项目的核心。 Here, it easy to see that torch is the core component of the project. Once you understand torch, you may start to check other APIs inherited from it. Now, how should we understand other APIs? Please check the next section. 这里的核心很明显就是torch，当你理解了torch。再开始找最需要理解的下一个继承者，很明显torch.nn需要搞明白。至于其他继承者们，应该怎么学呢？请看下面的分析。 3 分类讨论和按需理解如果你也需要和有时间去了解这些继承者们。除了搞明白官方文档，还需要看书，上课，写代码。这里，我教同学一招如何快速搞明白官方文档。快速搞明白官方文档分两步，具体操作如下。 3.1 分类讨论上面列出的一堆继承者们。大致可以分为下面这个几类（根据个人理解分类，没有绝对的正确）。分类的目的，是帮助同学快速逆向推理出项目的大致架构，从而可以快速理解项目的开发过程，进而可以更准确的根据需要理解对应的文档。 基本函数 torch.nntorch.nn.functionaltorch.nn.inittorch.autogradtorch.optim 进阶函数 torch.randomtorch.sparsetorch.StorageQuantization 基本数据结构 Tensor AttributesTensor Views 进阶数据结构 Type InfoNamed TensorsNamed Tensors operator coverage 基本数据存储 torch.cudatorch.cuda.amp 并行和分布式计算 torch.distributedtorch.distributionsDistributed RPC Framework 模型库 torch.hub 模型转换和调用 torch.jittorch.onnx 常用工具箱 torch.utils.bottlenecktorch.utils.checkpointtorch.utils.cpp_extensiontorch.utils.datatorch.utils.dlpacktorch.utils.model_zootorch.utils.tensorboard 各种深度学习任务数据接口 torchaudiotorchtexttorchvision 集群训练和服务 TorchElasticTorchServe 3.2 按需理解3.2.1 例子-线性回归经过上一轮分析，基本函数肯定要要搞明白的。这里面有： torch.nn torch.nn.functional torch.nn.init torch.autograd torch.optim 这部分我就用一个最简单的例子 - 线性回归，来帮助同学逐步理解这些PyTorch的基本函数。 机器学习有五大问题：分类、回归、聚类、降维、和强化。应用最广的可能就属回归分析了，其目的是帮助人们了解在只有一个自变量变化时因变量的变化量。在回归分析中，线性回归可谓是任何一门机器学习课程或者书籍的第一课，也是最简单和最先需要讲的算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import torchimport numpy as npimport matplotlib.pyplot as pltimport torch.optim as optimimport torch.nn as nnfrom PIL import Image# Create datax = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) noise = 5 * torch.rand(x.size()) bias = 5 * torch.ones(x.size())y = bias + 5 * x + noise# View data plt.title(&#x27;Linear Regression&#x27;, fontsize=20)plt.xlabel(&#x27;x&#x27;, fontsize=15)plt.ylabel(&#x27;y&#x27;, fontsize=15)plt.scatter(x.data.numpy(), y.data.numpy(), color = &quot;purple&quot;)plt.show()# Create modellinear_regression = nn.Linear(1, 1)# Start trainingoptimizer = optim.SGD(linear_regression.parameters(), lr=0.2)criterion = nn.MSELoss() fig, ax = plt.subplots(figsize=(12,7))images_list = []for epoch in range(100): y_pred = linear_regression(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() # plot and show learning process plt.cla() ax.set_title(&#x27;Linear Regression&#x27;, fontsize=20) ax.set_xlabel(&#x27;x&#x27;, fontsize=15) ax.set_ylabel(&#x27;y&#x27;, fontsize=15) ax.set_xlim(-1.5, 1.5) ax.set_ylim(-1.0, 16.0) ax.scatter(x.data.numpy(), y.data.numpy(), color = &quot;purple&quot;) ax.plot(x.data.numpy(), y_pred.data.numpy(), &#x27;g-&#x27;, lw=3) ax.text(0.8, 1.0, &#x27;Epoch = &#123;&#125;, Loss = &#123;:.2f&#125;&#x27;.format(epoch, loss.data.numpy()), fontdict=&#123;&#x27;size&#x27;: 10, &#x27;color&#x27;: &#x27;black&#x27;&#125;) fig.canvas.draw() img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#x27;uint8&#x27;) img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,)) img = Image.fromarray(img) images_list.append(img) # save images as a gif images_list[0].save(&#x27;./linear_regression.gif&#x27;, save_all=True, append_images=images_list[1:], optimize=False, duration=100, loop=0) 上面是一个完整展示，用PyTorch中的基本函数来解决一个线性回归分析。其中我用到torch.nn中的Linear来构建一个线性回归的算法模型（行22）。然后用torch.optim中的SGD建立一个优化方法（行25）。最后用，torch.nn中的MSELoss来定义损失函数（26行）。利用Pytoch训练定义好的模型和参数非常简单，只需要六行就可以搞定（上面行30到行35的代码）。剩下的代码都是做可视化用的，这个同学不需要理解。 为了让同学更直观的看到训练过程，我将输入变量（特征值）的维度设置成1维。并且，我把整个训练过程可视化出来。 线性回归分析简单来讲就是在一个线性分布的数据中，找到一个线（如图中的绿色的线），来拟合整个数据分布。代码中（9行到12行），我创建了一个线性分布的数据根据下面的式子（其中我加了一些高斯噪音，为了模拟真实生活中的数据）。 $$\\begin{equation}y = 5 + 5 x\\end{equation}$$ 所以要拟合的的函数就是： $$\\begin{equation}h_\\theta(x) = \\theta_0 + \\theta_1 x\\end{equation}$$ 看到这个公式，很多同学会想到: $$\\begin{equation}y = b + k x\\end{equation}$$ 没错，这就是我们小学课堂上面讲的一元一次函数。是不是瞬间感觉高大上的线性回归也不过如此。但是，实际生活中的数据并不是完美落在这条线上面的，而会像动图中的点一样，分布在这条线的两边。所以，给定一些点（训练数据）去找这条线的方法（模型训练）就是，找到一条线离各个点的距离最小。这里，衡量一条线到所有点的距离就叫损失，而去找离各个点距离最小的线就是去找损失函数最小的解，也就是估计上面公式中参数（b, k）。其最终的高维的数学表达式如下： $$\\begin{equation}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x_i)) - (y_i))^2\\end{equation}$$ 通过训练得到损失函数最小的解，就有可能得到原始函数（假设）的参数。最后可以将下面式子中所有的参数都换成了具体的数字： $$\\begin{equation}h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + … + \\theta_n x_n \\space \\space where \\space x_0 = 1\\end{equation}$$ 上面提供的代码例子其实有很多缺点。第一，不易维护。第二，不易拓展。第三，不易开发。所以，我们这里需要把，数据的载入封装起来，通过继承torch.utils.data中的Dataset来实现。 1234567891011121314151617import torchfrom torch.utils.data import Datasetclass SimpleData(Dataset): def __init__(self): self.x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) noise = 5 * torch.rand(self.x.size()) bias = 5 * torch.ones(self.x.size()) self.y = bias + 5 * self.x + noise self.len = self.y.shape[0] def __len__(self): return self.len def __getitem__(self, index): return self.x[index] , self.y[index] 然后再把模型的建立也封装起来，通过继承torch.nn中的Module来实现。 1234567891011import torch.nn as nnclass LinearRegression(nn.Module): def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): out = self.linear(x) return out 最后代码就被精简成这个样子（这里，我去掉可视化数据的代码） 1234567891011121314151617181920212223#.... Omit above visualization code!# Load datasimple_data = SimpleData()train_loader = DataLoader(dataset=simple_data, batch_size=100)# Create modellinear_regression = LinearRegression()# Start trainingoptimizer = optim.SGD(linear_regression.parameters(), lr=0.2)criterion = nn.MSELoss() fig, ax = plt.subplots(figsize=(12,7))images_list = []for epoch in range(100): for x, y in train_loader: y_pred = linear_regression(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step()#...Omit blow visualization code! 这样修改的代码可以完美解决上面我说的三个问题：不易维护、不易拓展、和不易开发。同时，通过调整DataLoader中参数batch_size的大小，可以随意切换训练方式（梯度下降、随机梯度下降、和小批量随机梯度下降）。最后，有一些基础知识的同学就能看出代码里还有一个问题：缺少验证数据集和测试数据集。这里，我只是像展示PyTorch这些基本函数的用法。如果，同学有兴趣，可以自己创造一个验证数据集和测试数据集，将其加入到整个模型训练和测试的流水线中。 3.2.2 例子-逻辑回归经过上一个例子，同学们应该对PyTorch的基本函数有所了解了。现在，我们就换一个例子，用逻辑回归来进一步了解PyTorch的基本函数。 机器学习有五大问题：分类、回归、聚类、降维、和强化。除了回归分析的应用最广，分类问题也是生活中遇到最多的问题之一。与回归分析不同，分类问题的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。根据类别的数量还可以进一步将分类问题划分为二元分类和多元分类。同样，在分类问题中，最基本的算法就应当是逻辑回归了 这里，我们用到一个最简单的分类问题数据集-安德森鸢尾花卉数据集。同学们可以从这个链接下载这个数据集。同样，这里我需要把数据的载入封装起来，通过继承torch.utils.data中的Dataset来实现。 123456789101112131415161718import torchimport numpy as npimport pandas as pdfrom torch.utils.data import Datasetclass IrisData(Dataset): def __init__(self): iris_data = pd.read_csv(&#x27;./Iris.csv&#x27;) self.x = torch.from_numpy(iris_data.loc[: 99, [&#x27;SepalLengthCm&#x27;, &#x27;SepalWidthCm&#x27;]].to_numpy(dtype=np.float32)) self.y = torch.cat([torch.zeros(50), torch.ones(50)]).view(-1, 1) self.len = self.y.shape[0] def __len__(self): return self.len def __getitem__(self, index): return self.x[index] , self.y[index] 然后再把模型的建立也封装起来，通过继承torch.nn中的Module来实现。 123456789101112import torch.nn as nnclass LogisticRegression(nn.Module): def __init__(self): super(LogisticRegression, self).__init__() self.linear = nn.Linear(2, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): out = self.sigmoid(self.linear(x)) return out 最后完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport torch.optim as optimimport torch.nn as nnfrom torch.utils.data import DataLoaderfrom PIL import Image# Process datairis_data = pd.read_csv(&#x27;./Iris.csv&#x27;)setosa = iris_data.loc[: 49][&#x27;SepalLengthCm&#x27;].to_numpy(dtype=np.float32), iris_data.loc[: 49][&#x27;SepalWidthCm&#x27;].to_numpy(dtype=np.float32)versicolor = iris_data.loc[50:99][&#x27;SepalLengthCm&#x27;].to_numpy(dtype=np.float32), iris_data.loc[50:99][&#x27;SepalWidthCm&#x27;].to_numpy(dtype=np.float32)# View dataplt.title(&#x27;Logistic Regression&#x27;, fontsize=20)plt.xlabel(&#x27;SepalLength (cm)&#x27;, fontsize=15)plt.ylabel(&#x27;SepalWidth (cm)&#x27;, fontsize=15)plt.scatter(setosa[0], setosa[1], color=&#x27;red&#x27;, label=&#x27;setosa&#x27;)plt.scatter(versicolor[0], versicolor[1], color=&#x27;blue&#x27;, label=&#x27;versicolor&#x27;)plt.legend(loc=1, fontsize=&#x27;x-large&#x27;)plt.show()# Load datalris_data = IrisData()train_loader = DataLoader(dataset=lris_data, batch_size=100)# Create Modellogistic_regression = LogisticRegression()# Start trainingcriterion = nn.BCELoss() optimizer = optim.SGD(logistic_regression.parameters(), lr=0.2)fig, ax = plt.subplots(figsize=(12,7))images_list = []for epoch in range(100): for x, y in train_loader: y_pred = logistic_regression(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() # Plot and show training process w0, w1 = logistic_regression.linear.weight.data.numpy()[0] b = logistic_regression.linear.bias.data.numpy()[0] plot_x = np.linspace(4, 8, 1000) plot_y = (-w0 * plot_x - b) / w1 plt.cla() ax.set_title(&#x27;Logistic Regression&#x27;, fontsize=20) ax.set_xlabel(&#x27;SepalLength (cm)&#x27;, fontsize=15) ax.set_ylabel(&#x27;SepalWidth (cm)&#x27;, fontsize=15) ax.set_xlim(3.5, 8.5) ax.set_ylim(1.0, 5.0) ax.scatter(setosa[0], setosa[1], color=&#x27;red&#x27;, label=&#x27;setosa&#x27;) ax.scatter(versicolor[0], versicolor[1], color=&#x27;blue&#x27;, label=&#x27;versicolor&#x27;) ax.plot(plot_x, plot_y, &#x27;g-&#x27;, lw=3) ax.legend(loc=1, fontsize=&#x27;x-large&#x27;) ax.text(7.5, 1.5, &#x27;Epoch = &#123;&#125;, Loss = &#123;:.2f&#125;&#x27;.format(epoch, loss.data.numpy()), fontdict=&#123;&#x27;size&#x27;: 10, &#x27;color&#x27;: &#x27;black&#x27;&#125;) fig.canvas.draw() img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#x27;uint8&#x27;) img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,)) img = Image.fromarray(img) images_list.append(img) # save images as a gif images_list[0].save(&#x27;./logistic_regression.gif&#x27;, save_all=True, append_images=images_list[1:], optimize=False, duration=100, loop=0) 上面的代码就是用PyTorch中的基本函数来解决一个逻辑回归问题。在LogisticRegression这个封装好的类里面，用到torch.nn中的Linear来先构建一个线性算子（行6），加上一个Sigmoid函数（行7），来构建一个逻辑回归算法模型。从实现逻辑回归算法模型的过程中，同学们可以清楚的看到逻辑回归就是线性回归套了一个Sigmoid函数的马甲。也可以说是线性回归站到了线性回归的肩膀上，看清楚了分类问题的本质。 逻辑回归被很多人认为是处理回归问题的算法，但其实都被他的名字所欺骗。这个算法其实是用来处理分类问题的，为什么叫逻辑回归不叫逻辑分类。其原因是可以从统计和数学两个角度来解释其联系。从数学上，逻辑回归本质上是线性回归的延申，线性回归加了一个Sigmoid函数来处理线性可分的问题。从统计上，两个模型都属于广义线性模型，线性回归模型中要假设随机误差等方差并且服从正态分布，而逻辑回归需要假设随机变量的参数服从伯努利分布 同样，我用torch.optim中的SGD建立一个优化方法（行32）。最后用，torch.nn中的BCELoss来定义损失函数（行31）。利用Pytoch训练定义好的模型和参数非常简单，只需要七行就可以搞定（上面行36到行42的代码）。剩下的代码都是做可视化用的，这个同学不需要理解。 为了让同学更直观的看到训练过程，我将输入变量（特征值）的维度设置成2维，并且处理的是一个二元分类问题。并且，我把整个训练过程可视化出来。 逻辑回归问题简单来讲就是在一个线性可分的数据中，找到一个线（如图中绿色的线），可以把两类数据分开。在IrisData这个封装好的类中，我抽取了两种花的数据（Setosa和versicolor）。 其中，红色点代表一类花，蓝色的点代表另一类花。萼片长度为X轴和萼片宽度为Y轴。从中的数据可以看出，这个数据在这两个维度上是线性可分的。 第二节的公式（1）可以作为二元分类问题的损失函数。那么，逻辑回归的原始函数（假设）和参数应该怎么表达呢？其数学公式如下。 $$\\begin{equation}h_\\theta(x) = \\sigma(\\theta_0 x_0 + \\theta_1 x_1 + … + \\theta_n x_n) \\space \\space where \\space x_0 = 1\\end{equation}$$ 从逻辑回归的数学表达式子中，可以看出其输出的是可能性(因为Sigmoid函数的值域是0到1)。所以需要进一步根据下面公式，输出具体的判定（0代表不是或者1代表是） $$\\begin{equation}y = \\begin{cases} 1 &amp; \\text{if } h_\\theta(x) \\geq 0.5 \\\\ 0 &amp; \\text{if } h_\\theta(x) &lt; 0.5\\end{cases}\\end{equation}$$ 最后，同3.2.1一样，这里缺少验证数据集和测试数据集。感兴趣同学有兴趣，可以自己分一个训练数据集、验证数据集、和测试数据集，然后将其加入到整个模型训练和测试的流水线中。 3.2.3 例子-神经网络PyTorch是一个非常优秀的深度学习框架，而深度学习可以说是最简单的神经网络的延申。所以，我这里就用一个简单的神经网络做为例子，继续帮助学生理解这些PyTorch的基本函数。 人工神经网络是一种计算模型，启发自人类大脑处理信息的生物神经网络。其本质上是多层感知器，也就是多层多路输出线性回归加一个激活函数。通过这个激活函数，每一路的输出最终形成一个神经元。这里，同一级众多的神经元最后形成了一层神经网络。其中第一层叫做输入层，中间所有层叫做隐藏层，最后一层叫做输出层。由于神经网络可以拟合任何函数，所以只需要将最后一个输出层替换成不同的结构，就可以处理各种机器学习问题 下面我们就用PyTorch是实现一个神经网络来处理回归分析。由于神经网络可以拟合任何函数， 我这里利用一个非线性函数生成一个非线性的数据分布，看看这个神经网络可否帮我拟合这个函数。首先，我们还是需要将数据的载入封装起来，通过继承torch.utils.data中的Dataset来实现。 12345678910111213141516import torchfrom torch.utils.data import Datasetclass ComplexeData(Dataset): def __init__(self): self.x = torch.unsqueeze(torch.linspace(-10, 10, 100), dim=1) noise = 0.5 * torch.rand(self.x.size()) self.y = torch.sin(self.x) + noise self.len = self.y.shape[0] def __len__(self): return self.len def __getitem__(self, index): return self.x[index] , self.y[index] 然后再把模型的建立也封装起来，通过继承torch.nn中的Module来实现。 12345678910111213141516import torch.nn as nnclass NeuralNetworks(nn.Module): def __init__(self): super(NeuralNetworks, self).__init__() self.hidden_1 = nn.Linear(1, 200) self.hidden_2 = nn.Linear(200, 100) self.output = nn.Linear(100, 1) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.hidden_1(x)) x = self.relu(self.hidden_2(x)) out = self.output(x) return out 最后完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import torchimport numpy as npimport matplotlib.pyplot as pltimport torch.optim as optimimport torch.nn as nnfrom torch.utils.data import DataLoaderfrom PIL import Image# Create datax_train = torch.unsqueeze(torch.linspace(-10, 10, 100), dim=1)noise = 0.5 * torch.rand(x_train.size()) y_train = torch.sin(x_train) + noise # View data plt.title(&#x27;Neural Networks Regression Analysis&#x27;, fontsize=20)plt.xlabel(&#x27;x&#x27;, fontsize=15)plt.ylabel(&#x27;y&#x27;, fontsize=15)plt.scatter(x_train.data.numpy(), y_train.data.numpy(), color = &quot;purple&quot;)plt.show()# Load datacomplex_data = ComplexeData()train_loader = DataLoader(dataset=complex_data, batch_size=100)# Create modelneural_networks = NeuralNetworks()# Start trainingoptimizer = optim.Adam(neural_networks.parameters(), lr=0.01)criterion = nn.MSELoss() fig, ax = plt.subplots(figsize=(12,7))images_list = []for epoch in range(1000): for x, y in train_loader: y_pred = neural_networks(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() # plot and show learning process plt.cla() ax.set_title(&#x27;Neural Networks Regression Analysis&#x27;, fontsize=20) ax.set_xlabel(&#x27;x&#x27;, fontsize=15) ax.set_ylabel(&#x27;y&#x27;, fontsize=15) ax.set_xlim(-10.5, 10.5) ax.set_ylim(-1.5, 2) ax.scatter(x_train.data.numpy(), y_train.data.numpy(), color = &quot;purple&quot;) ax.plot(x_train.data.numpy(), y_pred.data.numpy(), &#x27;g-&#x27;, lw=3) ax.text(6.0, -1.0, &#x27;Epoch = &#123;&#125;, Loss = &#123;:.2f&#125;&#x27;.format(epoch, loss.data.numpy()), fontdict=&#123;&#x27;size&#x27;: 10, &#x27;color&#x27;: &#x27;black&#x27;&#125;) fig.canvas.draw() img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#x27;uint8&#x27;) img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,)) img = Image.fromarray(img) images_list.append(img) # save images as a gif images_list[0].save(&#x27;./neural_networks_regression_analysis.gif&#x27;, save_all=True, append_images=images_list[1:], optimize=False, duration=100, loop=0) 上面是的代码，我用PyTorch中的基本函数来构造一个简单神经网络，去解决复杂的非线性回归分析。在NeuralNetworks这个封装好的类里面，利用torch.nn中的Linear构建一个输入层（行6），利用torch.nn中的Linear构建一个隐藏层（行7），利用torch.nn中的Linear构建一个输出层（行8）。同时，需要定义一个激活函数，利用torch.nn中的ReLU来实现（行9）。最后在forward函数中，把定义好的各种层和激活函数依次串起来。 接下来，我用torch.optim中的Adam建立一个优化方法（行29）。最后用，torch.nn中的MSELoss来定义损失函数（26行）。利用Pytoch训练定义好的模型和参数非常简单，只需要七行就可以搞定（上面行34到行40的代码）。剩下的代码都是做可视化用的，这个同学不需要理解。 为了让同学更直观的看到训练过程，我将输入变量（特征值）的维度设置成1维。并且，我把整个训练过程可视化出来。 在3.2.1中，我解释过线性回归分析的算法。同理，这里用神经网络做回归分析，只需要将最后一个输出层改成输出的具体数值即可。这样就可以用来分析一个非线性分布的数据。从而找到一条线（如图中的绿色的线），来拟合整个数据分布。在ComplexeData这个封装好的类中, 我用下面的式子（其中我加了一些高斯噪音，为了模拟真实生活中的数据），创造了一个非线性的数据分布。 $$\\begin{equation}y = \\sin(x)\\end{equation}$$ 所以要拟合的的函数就是： $$\\begin{equation}h_\\theta(x) = \\sin(x)\\end{equation}$$ 因为还是回归分析，所以损失函数还是去衡量一条线到所有点的距离就叫损失。也就是，给定一些点（训练数据）去找这条线的方法（模型训练）就是，找到一条线离各个点的距离最小。所以其数学表达式还是： $$\\begin{equation}J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\Theta(x_i)) - (y_i))^2\\end{equation}$$ 由于神经网络是一个黑盒子，我们无法具体知道可能得到原始函数（假设）会是什么。所以，原始函数的参数也不知道是什么。但是我们每次训练好神经网络会产生一个所有神经元的权重信息。大部分情况下，这些权重信息并不能说明什么问题（现在很多研究开始尝试可视化这些权重信息，并且去尝试解释神经网络认为哪些是重要的信息）。 在这节给出的例子中，我们知道生成这些数据的原始函数是什么。同时，这些数据的维度只有一个。所以，通过上面可视化训练过程，我们可以很直观的知道，这个神经网络是很好的拟合了原始函数（假设）。 在实际生活中，很多非结构数据过于复杂，使得我们无法得知其真实的分布应该是什么。所以，我们无法正确的猜测其假设。因此，就需要一个更加庞大的训练集、验证集、和测试集去判定一个模型表现的好与坏。但是，即使有些神经网络在测试集上表现很好，也不会代表这个模型在实际生产和生活中表现的很好。个人认为，在实际生产和生活中，数据的分布可能不止一种。同时，没有通用的方法可以判定，这个数据或这类数据是什么什么分布。最重要，数据的收集过程和数据的标记会不会有很大的人为偏差？ 构建神经网络还有PyTorch还提供了另一种更加简单的方式。利用torch.nn中的Sequential可以将NeuralNetworks这个类替换掉。所以，我们只需要一个ComplexeData的类负责加载数据集就好了。 12345678910111213141516import torchfrom torch.utils.data import Datasetclass ComplexeData(Dataset): def __init__(self): self.x = torch.unsqueeze(torch.linspace(-10, 10, 100), dim=1) noise = 0.5 * torch.rand(self.x.size()) self.y = torch.sin(self.x) + noise self.len = self.y.shape[0] def __len__(self): return self.len def __getitem__(self, index): return self.x[index] , self.y[index] ComplexeData没有什么改动。但是，NeuralNetworks被换成了如下（这里，我去掉可视化数据的代码） 12345678910111213141516171819202122232425262728#.... Omit above visualization code!# Load datacomplex_data = ComplexeData()train_loader = DataLoader(dataset=complex_data, batch_size=100)# Create modelneural_networks = nn.Sequential( nn.Linear(1, 200), nn.ReLU(), nn.Linear(200, 100), nn.ReLU(), nn.Linear(100, 1))# Start trainingoptimizer = optim.Adam(neural_networks.parameters(), lr=0.01)criterion = nn.MSELoss() fig, ax = plt.subplots(figsize=(12,7))images_list = []for epoch in range(1000): for x, y in train_loader: y_pred = neural_networks(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step()#...Omit blow visualization code! 这两种建立神经网络都一样，但是不同的方法有不同的用户使用群。同理，torch.nn.functional 和 torch.nn也类似。另外，torch.nn.init为科研人员提供了更为自由的权重初始化API。最后基础函数中的torch.autograd提供了类和函数用来对任意标量函数进行求导。要想使用自动求导，只需要对已有的代码进行微小的改变，并且将所有的tensor包含进Variable对象中即可。 3.2.4 深度神经网络深度神经网络可以看作是神经网络的进一步延申。深度神经网络拥有更多隐藏层，每一层的神经元也更多。同时，深度神经网络还有更复杂的结构，来处理各种不同的数据（例如，音频，文本，图片）。例如，卷积神经网络（CNN），专门用于处理图片数据；而循环神经网络（RNN），则更适合去处理文本数据。现在随着科技的发展，以及各行各业对AI的需求加大。更多类型的数据变的越来越流行（例如，点云），对应的深度神经网络结构也有很大变化。如果要继续细化深度神经网络，还可以根据所处理的不同问题（例如，音频的音频分类，文本的机器翻译，图像的目标检测），在结构上有千变万化的组合。 最后，很多机器/深度学习的知识搞死记硬背是行不通的。需要自己同手推导公式，自己动手用代码实现公式。那么，PyTorch做为这么优秀的开源学习软件，为什么不去自己试试呢？用它来实现各种算法和网络，会不会是一件很酷的事情呢？ 4 站在巨人肩上眺望根据第三节中的三个例子，希望同学们能对PyTorch有一个大致的了解。同时从上面的三个例子中，可以看出PyTorch作为优秀的深度学习的框架，在搭建各种各样的深度神经网络可谓是非常简单。这里，也希望同学可以根据所处理和研究的问题，搭建起属于自己的深度神经网络。 分析完基本函数，我们再来看基本数据结构。之前我说过，PyTorch之所以如此成功，是因为它站在了NumPy的肩膀是。为什么这个说呢？PyTorch中储存数据的基本单元叫tensor，而NumPy叫做ndarray。 123456789101112import torchimport numpy as nparray_list = [[11, 12, 13], [21, 22, 23], [31, 32, 33]]pytorch_tensor = torch.tensor(array_list)numpy_ndarray = np.array(array_list)print(&quot;Create PyTorch Tensor is: &#123;&#125;&quot;.format(pytorch_tensor))print(&quot;Create NumPy Ndarray is: &#123;&#125;&quot;.format(numpy_ndarray))print(&quot;Convert PyTorch Tensor to NumPy Ndarray is same: &#123;&#125;&quot;.format(pytorch_tensor.numpy()==numpy_ndarray))print(&quot;Convert NumPy Ndarray to PyTorch Tensor is same: &#123;&#125;&quot;.format(pytorch_tensor==torch.from_numpy(numpy_ndarray)))print(&quot;The data type in PyTorch Tensor is: &#123;&#125;&quot;.format(pytorch_tensor.dtype))print(&quot;The data type in NumPy Ndarray is: &#123;&#125;&quot;.format(numpy_ndarray.dtype)) 在上面的例子中，PyTorch的tensor不仅可以和NumPy中的ndarray相互切换，访问数据类型的函数都是一样的。更重要的是，NumPy中有的对ndarray的各种操作，PyTorch中的tensor也都一摸一样。这时很多同学就会反问了：“这不是基本操作吗？”对，正式因为基本操作，PyTorch就直接拿来一用，这不香吗？虽然，NumPy底层是用C实现的，而PyTorch大多功能是用C++实现的。但是，用什么方式和语言实现都是次要，关键是PyTorch模仿了很多NumPy东西。比如功能设计的非常人性，运算速度快，误差极小，从数据结构到各种算法的优化方式。最最最重要的是ndarray支持并行化运算（向量化运算），而且底层使用C语言编写，内部解除了GIL（全局解释器锁），其对数组的操作速度不受Python解释器的限制。这些NumPy的优点使得PyTorch去实现很多设计理念变的非常容易。所以站在巨人肩膀上的方式，不是简单的复制粘贴被人的东西；而是，借鉴巨人的设计思路，模仿巨人的实现方式。 最后PyTorch站在NumPy的肩膀上，一举横扫诸深度神经网络框架。那你是否想站在PyTorch的肩膀上，去探索深度学习的世界？ 5 Note 最后寄语大量练习写PyTorch代码！很多东西靠看是学不会的，自动动手写一写。哪怕重写一些教程都比死记硬背的要好的多。 在创业公司工作，每年都会遇到一到两个大型开源项目，时不时还有很多小的开源项目。在FLAG工作也一样，虽然基本都在做公司的东西，但是也需要经常去学习和参考别人是怎么做的，有什么新的设计理念，有什么新的技术，有什么新功能需要实现。所以，希望同学可以掌握或者总结一套自己的学习方法，去面对未来高速发展的社会。 再次感谢这些做开源项目的人！致敬！ 6 Study materials 学习资料 PyTorch Documentation PyTorch Tutorials Deep Learning with PyTorch Deep Neural Networks with PyTorch PyTorch Examples PyTorch Discuss 7 Reference 引用 View All 查看全部 PyTorch: An Imperative Style, High-Performance Deep Learning Library, Adam Paszke et al., NeurIPS, 2019 - paper link Regression with Neural Networks in PyTorch, Ben Phillips, Medium - link"}],"posts":[{"title":"Biography","slug":"biography","date":"2020-10-02T03:17:52.128Z","updated":"2020-10-02T03:17:52.128Z","comments":true,"path":"2020/10/01/biography/","link":"","permalink":"https://weikunhan.github.io/2020/10/01/biography/","excerpt":"","text":"About Weikun Han is currently working as a research assistant for Dr. Fuxin Li on 3D Point Cloud algorithms at OSU. Weikun Han was a computer vision scientist at Clobotics where he worked on multiple computer vision tasks (image classification, object detection, semantic segmentation). Besides modeling to many real-life challenging problems, he was a machine learning engineer to work on many machine learning system design tasks (ceiling analysis dashboard, active learning pipeline, image retrieval system, distributed data pipeline, online learning pipeline), and he was a software development engineer to work on many infrastructure development tasks (product search website, data operations platform). Weikun Han received an M.S. degree in electrical and computer engineering from UCLA, supervised by Prof. Lei He. He received a B.S. degree in electrical engineering from ISU, supervised by Prof. Liang Dong. He also received the IEEE NANO best student paper award, and he co-authored 5 publications in major journals and conferences during the undergraduate research study. Interests Machine Learning Deep Learning Computer Vision Education 2018 M.S., Electrical and Computer Engineering, University of California Los Angeles 2016 B.S., Electrical Engineering, Iowa State University,","categories":[],"tags":[]},{"title":"News","slug":"news","date":"2020-09-23T20:03:18.843Z","updated":"2020-09-23T20:03:18.843Z","comments":true,"path":"2020/09/23/news/","link":"","permalink":"https://weikunhan.github.io/2020/09/23/news/","excerpt":"","text":"News 7/1/2020 learn more I will work as a research assistant for Dr. Fuxin Li on 3D Point Cloud algorithms. 6/1/2020 learn more I will join Dr. Fuxin Li’s research group as research assistant intern at Collaborative Robotics and Intelligent Systems (CoRIS) Institute. 5/29/2020 learn more I will resign from Clobotics Global. I am actively looking for a research group to keep proceeding with my Ph.D. study.","categories":[],"tags":[]}],"categories":[],"tags":[]}