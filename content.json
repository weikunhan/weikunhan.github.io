{"meta":{"title":"Weikun Han's Website","subtitle":"","description":"","author":"Weikunu Han","url":"https://weikunhan.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-05-30T08:38:52.382Z","updated":"2020-05-30T08:38:52.382Z","comments":true,"path":"404.html","permalink":"https://weikunhan.github.io/404.html","excerpt":"","text":"404 Sorry The address may be incorrect or may have been deleted"},{"title":"Awards","date":"2020-05-30T08:38:52.385Z","updated":"2020-05-30T08:38:52.385Z","comments":true,"path":"awards/index.html","permalink":"https://weikunhan.github.io/awards/index.html","excerpt":"","text":"Best Paper Award Best Student Paper Award, IEEE NANO, 2017, Tape-based flexible metallic and dielectric nanophotonic devices and metamaterials Others Magna Cum Laude Honor, Iowa State University, 2015, Ranked 1st in a class of 2015 Electrical Engineering graduates (Winter) Dean’s List Honor, Iowa State University, 2013 - 2015, Given to top ranked students"},{"title":"","date":"2020-05-30T08:38:52.386Z","updated":"2020-05-30T08:38:52.386Z","comments":true,"path":"blogs/index.html","permalink":"https://weikunhan.github.io/blogs/index.html","excerpt":"","text":""},{"title":"Courses","date":"2020-06-03T01:10:45.020Z","updated":"2020-06-03T01:10:45.020Z","comments":true,"path":"courses/index.html","permalink":"https://weikunhan.github.io/courses/index.html","excerpt":"","text":"Teaching Signals and Systems I, Iowa State University, Teacher Assistant, Fall 2014 Selected Courses Machine Learning, Stanford University, Coursera Introduction to Self-Driving Cars, University of Toronto, Coursera Convex Optimization, University of California Los Angeles, Spring 2018 Large Scale Social and Complex Networks: Design and Algorithms, University of California Los Angeles, Spring 2017 Large-Scale Data Mining: Models and Algorithms, University of California Los Angeles, Winter 2017 Favored Books Aurélien Géron, Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, O’Reilly Media, 2017 Ian J. Goodfellow and Yoshua Bengio and Aaron Courville, Deep Learning, MIT press, 2016 Bill Chambers, Matei Zaharia, Spark: The Definitive Guide, O’Reilly Media, 2018 Jiawei Han, Micheline Kamber, Jian Pei, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, 2012 Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford, Introduction to Algorithms, 2001"},{"title":"Projects","date":"2020-06-03T01:03:39.465Z","updated":"2020-06-03T01:03:39.465Z","comments":true,"path":"projects/index.html","permalink":"https://weikunhan.github.io/projects/index.html","excerpt":"","text":"Research Projects Coming soon Coming soon Coming soon 0pen Source Projects Tripadvisor Web Crawler Individual Projects LeetCode Python Coming soon"},{"title":"Publications","date":"2020-05-30T08:38:52.390Z","updated":"2020-05-30T08:38:52.390Z","comments":true,"path":"publications/index.html","permalink":"https://weikunhan.github.io/publications/index.html","excerpt":"","text":"Selected Journal Publications Tape nanolithography: a rapid and simple method for fabricating flexible, wearable nanophotonic devicesQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongMicrosystems and Nanengineering, 4, 31, 2018 See full publication list Selected Conference Publications Tape-based flexible metallic and dielectric nanophotonic devices and metamaterialsQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongIEEE 17th International Conference on Nanotechnology (IEEE-NANO), 2017 Best Student Paper Award See full publication list"},{"title":"Wrote before deciding to proceed my Ph.D.study","date":"2020-04-29T07:00:00.000Z","updated":"2020-05-30T08:38:52.387Z","comments":true,"path":"blogs/phd_study/index.html","permalink":"https://weikunhan.github.io/blogs/phd_study/index.html","excerpt":"“⼈⽣过早的⾯对失败和挫折，要⽐不惑之年再想明⽩很多事情要好很多。“ - 韩劲谦","text":"“⼈⽣过早的⾯对失败和挫折，要⽐不惑之年再想明⽩很多事情要好很多。“ - 韩劲谦 Due to COVID-19, I have time to stop and think about what I should pursue in the next five years. I am lucky that can join one startup company (Clobotics Global), which force on use computer vision to provide end-end AI solution for global consumer goods companies. During two years of working in Clobotics Global, I was very honored to collaborate with excellent engineers and scientists to solve many real-life challenging problems. I had lots of great chances that could use what I learn from before to implement many ideas. I was lucky that I could be mentored by many excellent industry leaders. Here, I want to thank Dr. Yipeng Li, Dr. Feng Zhu, Dr. Albert Chen, and Dr. Yan Ke. At the same time, I feel there are still have a long way to go to let more and more AI-related products to walk into people’s life. For example, 1)related infrastructure development for AI-related products, 2)computation cost for model serving/training, 3)labeling cost for a supervised learning task, 4)uncertainty in the deep learning-based model for challenging computer vision tasks, 5)and more advanced and challenging real-life computer vision tasks. In the next decade, I believe people’s lifestyles will become more and more intelligent. Lots of great companies like Intel, Nvidia, and AMD are putting lots of sources on the research to provide more powerful and high-performance computing hardware. Google, Facebook, Amazon, and Microsoft are putting lots of sources on the research to provide more high-efficiency, convenient, and robust service. However, It is not enough and only a beginning, there are lots of problems that need a more effective way to solve it. Therefore, that is the reason why I want to back to the academy, dive deep, and generate a more great idea to solve those real-life challenging problems. I plan to conduct research in the following reaches area: Algorithm optimization in computer vision: Could we provide a more robust model in many other tasks such as Fast R-CNN in object detection? For example, there are many challenging subtasks in five main computer vision tasks (image classification, object detection, target tracking, semantic segmentation, instance segmentation) that need a more robust model to provide better performance. Learning method in computer vision: How could let machine study like people? Currently, people still use supervised learning as main training method for five main computer vision tasks in the industry. This training method requires tons of labeled data which cause extremely high cost for company operation. Is there exist a better way to do it? For example, “self-supervised learning” that would demonstrate more powerful compare with bringing humans into the loop for facilitating bias discovery in image datasets Advanced data in computer vision: Some challenging computer vision tasks may not solve by current popular technology. For example, base on 2D image data, how could let the machine confidently identify 16.9 fl oz Coca-Cola Soda with 12 fl oz Coca-Cola Soda from different perspectives? Therefore, I believe that we could involve 3D image data to solve those tricky problems. 2020年5⽉29号是我在公司的最后⼀天。由于疫情的原因，先后遇到⼯作、⽣活、健康⽅⾯的各种困难。正因为如此，我有时间停下来思考，未来的五年我应该追求的⽬标。 我有幸加⼊⼀家初创公司（扩博智能—借助计算机视觉为全球消费品公司提供端到端的AI解决⽅案）。在扩博智能⼯作期间，我很荣幸与优秀的⼯程师和科学家合作，利⽤计算机视觉技术解决很多现实⽣活中的难题，让生活更加智能。期间，我还有大量的机会可以利⽤以前学到的东⻄来实现许多自己想法。同时，我很幸运能得到许多杰出⾏业领袖的指导。 在这⾥，我要感谢Yipeng Li博⼠、Feng Zhu博⼠、Albert Chen博⼠和Yan Ke博⼠。 在近两年的⼯作实践中，我认为要让越来越多的AI相关产品进⼊⼈们的⽣活还有很⻓的路要⾛。 诸如：⼈⼯智能相关产品的基础设施开发；如何降低模型服务以及训练的成本；如何降低监督学习任务的数据标记成本；如何解决基于深度学习的计算机视觉模型的不确定性；以及解决更具挑战性的计算机视觉任务。 在未来的⼗年中，我相信⼈们的⽣活⽅式将变得更加智能。 许多伟⼤的公司，例如英特尔、英伟达和AMD，都在科研中投⼊了⼤量资源来开发功能更强⼤和性能更⾼的计算硬件。 ⾕歌，脸书和微软也都在研究中投⼊了⾮常多资源来提供更⾼效、更便捷、更加智能的服务。但是，这还远远不够，⽽且仅仅是⼀个开始，有许多问题需要更有效的⽅法来解决。 因此，我决定带着这些问题，开始为期5年的博⼠研究⼯作。希望在此期间，我可以对固有的基础知识有更扎实的理解、对AI相关领域有更深的看法、以及对越来越多的AI相关产品⾛⼊⽣活提供更好的解决⽅法。 最后，再次感谢公司近两年的栽培，也祝福⾃⼰在未来可以有更多的突破。 Contact Me GitHub LinkedIn Google Scholar Curriculum Vitae"},{"title":"Journal Publications","date":"2020-05-30T08:38:52.391Z","updated":"2020-05-30T08:38:52.391Z","comments":true,"path":"publications/journal_publications/index.html","permalink":"https://weikunhan.github.io/publications/journal_publications/index.html","excerpt":"","text":"Computer VisionMicroelectromechanical Systems2018 Tape nanolithography: a rapid and simple method for fabricating flexible, wearable nanophotonic devicesQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongMicrosystems and Nanengineering, 4, 31, 2018 2016 Strain-tunable plasmonic crystal using elevated nanodisks with polarization-dependent characteristicsYifei Wang, Longju Liu, Qiugu Wang, Weikun Han, Meng Lu and Liang DongApplied Physics Letters, 108, 071110, 2016 Electrically tunable quasi-3-D mushroom plasmonic crystalQiugu Wang, Weikun Han, Peng Liu, Liang DongJournal of Lightwave Technology, 34, 2175-2181 2016"},{"title":"Conference Publications","date":"2020-05-30T08:38:52.389Z","updated":"2020-05-30T08:38:52.389Z","comments":true,"path":"publications/conference_publications/index.html","permalink":"https://weikunhan.github.io/publications/conference_publications/index.html","excerpt":"","text":"Computer VisionMicroelectromechanical Systems2017 Tape-based flexible metallic and dielectric nanophotonic devices and metamaterialsQiugu Wang, Weikun Han, Yifei Wang, Meng Lu and Liang DongIEEE 17th International Conference on Nanotechnology (IEEE-NANO), 2017 Best Student Paper Award 2015 Strain-tunable two-dimensional plasmonic crystalsYifei Wang, Longju Liu, Qiugu Wang, Weikun Han, Meng Lu and Liang DongIEEE Photonics Conference (IPC), 2015"},{"title":"Introduce the fundamental method to learn Pytorch APIs","date":"2020-07-01T07:00:00.000Z","updated":"2020-07-11T18:57:38.642Z","comments":true,"path":"blogs/technologies_study/pytorch/index.html","permalink":"https://weikunhan.github.io/blogs/technologies_study/pytorch/index.html","excerpt":"“当你学会站在巨人的肩膀上，你突然发现可以看到更远的山川。“ - 韩劲谦","text":"“当你学会站在巨人的肩膀上，你突然发现可以看到更远的山川。“ - 韩劲谦 开门不见山随着深度学习发展，深度学习的框架也层出不穷。现在最流行的深度学习框架应该非PyTorch莫属了。有数据显示，在2020的CVPR论文中PyTorch占比是TensorFlow 4 倍。从公司退来之后，终于有机会接触PyTorch。上手PyTorch之后，只能用两个子形容：真香。 在过去工作的两年里，从令人跺脚的Caffe到后来快速稳定的TensorFlow，我一直对PyTorch带着有色眼镜。讲道理，之前的PyTorch还是更适合做研究，对于工业界的一整套东西（数据整合，模型训练，模型验证，模型服务）支持的并不是很理想。现在的PyTorch，可谓是深度学习框架的翘楚。首先，各种类的封装做的没话说，理解和自定义类非常简单。其次，API设计的非常人性，功能很全，使用起来非常方便。最后，官方文档写的很详细，官方教程很多，并且还有PyTorch社区帮忙回答各种问题。 PyTorch的接口是Python，但底层主要都是用C++实现的。PyTorch使用一种称之为 imperative/eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为define-by-run方法。- 机器之心 上面这段摘自机器之心对PyTorch基本描述看起来很高大上，其实对同学入门PyTorch并没有太大帮助。但是因为上面这个设计理念，让PyTorch变的清新脱俗。因为大部分入门PyTorch的同学主要关心如何快速上手，所以，这里我就不再对这些高大上的设计理念展开更多的介绍。 牛顿说过：“如果我看得更远一点的话，是因为我站在巨人的肩膀上”。 PyTorch之所以能变的如此流行，也是因为站在了巨人的肩上。PyTorch不是闭门造车，更不是从头一点一点开始。很多人说：“PyTorch的工作流程非常接近于Python的科学计算库NumPy”。如此说来，PyTorch站在了NumPy的肩膀上。后面的内容，我会说明为什么PyTorch站在了NumPy的肩膀上。 由于PyTorch各类教程很多，我在最后列出了一些PyTorch学习资源，希望同学们对这些学习资源各取所需。这篇博客，我以PyTorch为例子，主要介绍一种快速上手大型项目的方法。希望这样的学习方法可以帮助到一些学生。 拨开迷雾见月明现在，如果你对PyTorch非常感兴趣，让我带你来走进PyTorch世界。那么，如何快速上手PyTorch，应对实战项目呢？ 看官方文档 看开源PyTorch项目 看推荐PyTorch书籍 上精品PyTorch网课 大量练习写PyTorch代码！ 上面5点，最重要的是第四点。但是，一般来讲接触到一个陌生的东西，第一步是源自开源项目，然后开始看官方文档。那么现在就看一个例子 - 如何正确的自学成才。 12import torchimport torch.nn as nn 很多开源PyTorch项目会看到上面的API。这也是PyTorch最重要的两个APIs。第一个API是torch， 基本是NumPy换了一个马甲，但是同时增加很多一个深度学习的框架应该有的基本功能。可以说，正式因为torch，才组建了如此高大上的深度学习的框架。我们可以用下面的例子来说明，让我们试试如何用torch实现torch.nn的功能。 123456789101112import torchimport torch.nn as nnx = torch.tensor([-4.5, 0.7, 3.3])y = torch.tensor([1., 0., 1.])h = nn.Sigmoid()criterion = nn.BCELoss()torch_nn_loss = criterion(h(x), y)print('Logistic Regression Cross Entropy Loss use torch.nn: loss = &#123;&#125;'.format(torch_nn_loss.item()))torch_loss = -(y * torch.log(torch.sigmoid(x)) + (1 - y) * torch.log(1 - torch.sigmoid(x))).mean()print('Logistic Regression Cross Entropy Loss use torch: loss = &#123;&#125;'.format(torch_loss.item())) 上面的代码展示了一个分类问题损失函数 - 逻辑回归。其数学表达式如下： $$\\begin{equation}J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\lbrack y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\rbrack\\end{equation}$$ 最后结果两边的计算结果都是1.884。从这里开始很明显的看出：用torch基本的算子可以实现torch.nn的功能。看到这里，是不是感到很惊讶，感觉自己可以去做一个深度学习的框架了？别急，如果直接用torch的基本的算子实现的各种函数功能，可能需要写上万行多余的代码。而且这样的代码，对后期的维护、功能扩展、代码的性能优化、和其他硬软件的兼容都是灾难的铺垫！所以，PyTorch的开发人员利用编程中很重要的基本思想 - 面向对象编程。这个可能你都不清楚的基本思想，加上套了马甲的NumPy。不仅实现了torch.nn，还实现了： torch.nn.functional torch.Tensor Tensor Attributes Tensor Views torch.autograd torch.cuda torch.cuda.amp torch.distributed torch.distributions torch.hub torch.jit torch.nn.init torch.onnx torch.optim Quantization Distributed RPC Framework torch.random torch.sparse torch.Storage torch.utils.bottleneck torch.utils.checkpoint torch.utils.cpp_extension torch.utils.data torch.utils.dlpack torch.utils.model_zoo torch.utils.tensorboard Type Info Named Tensors Named Tensors operator coverage 以及： torchaudio torchtext torchvision TorchElastic TorchServe 上面是列出的所有1.5版本所有PyTorch的API。日后如果PyTorch继续发达了，请参考官方文档！一般同学看到这么复杂的文档，心情肯定是很糟糕的。但是，正如我最开始举的例子，它们都是继承者们，没错是torch的继承者们。所以，读到这里，你应该明白一件事情： 想要快速上手大型开源项目，首先抓住这个项目的核心。 这里的核心很明显就是torch，当你理解了torch。再开始找最需要理解的下一个继承者，很明显torch.nn需要搞明白。至于其他继承者们，需要的时候再花时间研究。作者上手PyTorch用了两天的时间，但是却花了大量的时间在他的继承者上面。 分类讨论和按需理解如果你也需要和有时间去了解这些继承者们。除了搞明白官方文档，还需要看书，上课，写代码。这里，我教同学一招如何快速搞明白官方文档。快速搞明白官方文档分两步，具体操作如下。 分类讨论上面列出的一堆继承者们。大致可以分为下面这个几类（根据个人理解分类，没有绝对的正确）。分类的目的，是帮助同学快速逆向推理出项目的大致架构，从而可以快速理解项目的开发过程，进而可以更准确的根据需要理解对应的文档。 基本函数 torch.nntorch.nn.functionaltorch.nn.inittorch.autogradtorch.optim 进阶函数 torch.randomtorch.sparsetorch.StorageQuantization 基本数据结构 Tensor AttributesTensor Views 进阶数据结构 Type InfoNamed TensorsNamed Tensors operator coverage 基本数据存储 torch.cudatorch.cuda.amp 并行和分布式计算 torch.distributedtorch.distributionsDistributed RPC Framework 模型动物园 torch.hub 转换和调用 torch.jittorch.onnx 工具箱 torch.utils.bottlenecktorch.utils.checkpointtorch.utils.cpp_extensiontorch.utils.datatorch.utils.dlpacktorch.utils.model_zootorch.utils.tensorboard 数据处理接口 torchaudiotorchtexttorchvision 集群训练和服务 TorchElasticTorchServe 按需理解例子-线性回归经过上一轮分析，基本函数肯定要要搞明白的。这里面有： torch.nn torch.nn.functional torch.nn.init torch.autograd torch.optim 这部分我就用一个最简单的例子 - 线性回归，来帮助同学逐步理解这些基本函数。 机器学习有五大问题：分类、回归、聚类、降维。其应用最广的可能就属回归分析了，再回归分析中，线性回归可谓是任何一门机器学习课程或者书籍必须要讲的，也是最先要讲的回归分析算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import torchimport numpy as npimport matplotlib.pyplot as pltimport torch.optim as optimimport torch.nn as nnfrom PIL import Image# Create datax = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) noise = 5 * torch.rand(x.size()) bias = 5 * torch.ones(x.size())y = bias + 5 * x + noise# View data plt.title('Linear Regression', fontsize=20)plt.xlabel('x', fontsize=15)plt.ylabel('y', fontsize=15)plt.scatter(x.data.numpy(), y.data.numpy(), color = \"purple\")plt.show()# Create modellinear_regression = nn.Linear(1, 1)# Start trainingoptimizer = optim.SGD(linear_regression.parameters(), lr=0.2)criterion = nn.MSELoss() fig, ax = plt.subplots(figsize=(12,7))images_list = []for epoch in range(100): y_pred = linear_regression(x) loss = criterion(y_pred, y) optimizer.zero_grad() loss.backward() optimizer.step() # plot and show learning process plt.cla() ax.set_title('Linear Regression', fontsize=20) ax.set_xlabel('x', fontsize=15) ax.set_ylabel('y', fontsize=15) ax.set_xlim(-1.5, 1.5) ax.set_ylim(-1.0, 16.0) ax.scatter(x.data.numpy(), y.data.numpy(), color = \"purple\") ax.plot(x.data.numpy(), y_pred.data.numpy(), 'g-', lw=3) ax.text(0.8, 1.0, 'Epoch = &#123;&#125;, Loss = &#123;:.2f&#125;'.format(epoch, loss.data.numpy()), fontdict=&#123;'size': 10, 'color': 'black'&#125;) fig.canvas.draw() img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8') img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,)) img = Image.fromarray(img) images_list.append(img) # save images as a gif images_list[0].save('./linear_regression.gif', save_all=True, append_images=images_list[1:], optimize=False, duration=100, loop=0) 上面是一个完整展示，用PyTorch中的基本函数来解决一个线性回归分析。其中我用到torch.nn中的Linear来用线性回归算法对一个回归问题行进建模 为了让同学更直观的看到训练过程，我这个的输入变量（特征值）的维度是1维。所以要拟合的的函数就是： $$\\begin{equation}h_\\theta(x) = \\theta_0 + \\theta_1 x\\end{equation}$$ 看到这个公式，很多同学会想到: $$\\begin{equation}y = b + k x\\end{equation}$$ 没错，这就是我们小学课堂上面讲的一元一次函数。是不是瞬间感觉高大上的线性回归也不过如此。但是，实际生活中的数据并不是完美落在这条线上面的，而会像动图中的点一样，分布在这条线的两边。所以，给定一些点（训练数据）去找这条线的方法（模型训练）就是，找到一条线离各个点的距离最小。这里，衡量一条线到所有点的距离就叫损失，而去找离各个点距离最小的线就是去找损失函数最小的解，也就是估计上面公式中参数（b, k）。其最终的数学表达式如下： $$\\begin{equation}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x_i)) - (y_i))^2\\end{equation}$$ 通过训练得到损失函数最小的解。就可以得到原式（假设）的参数，最后可以将下面式子中所有的参数都换成了具体的数字： $$\\begin{equation}h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + … + \\theta_n x_n \\space \\space where \\space x_0 = 1\\end{equation}$$ 例子-逻辑回归例子-神经网络人工神经网络是一种计算模型，启发自人类大脑处理信息的生物神经网络。其本质上是多层感知器，也就是多层的多路输出线性回归，每一层线性回归的每一个输出都有一个激活函数，通过这个激活函数，最终形成一个神经元。其中第一层叫做输入节点，中间所有层叫做隐藏节点，最后一层叫做输出节点。下面我们就用PyTorch是实现一个神经网络。 深度神经网络深度神经网络可以看作是神经网络的进一步延申。根据内部结构和所处理的问题（例如，自然语言和计算机视觉），可以分成各式各样的深度神经网络。所以根据上面的三个例子，希望同学们能对PyTorch有一个大致的了解。同时从上面的三个例子中，可以看出PyTorch作为优秀的深度学习的框架，在搭建各种各样的深度神经网络可谓是非常简单。这里，也希望同学可以根据所处理和研究的问题，搭建起属于自己的深度神经网络。 站在巨人肩上眺望分析完基本函数，我们再来看基本数据结构。之前我说过，PyTorch之所以如此成功，是因为它站在了NumPy的肩膀是。为什么这个说呢？下面看两个例子。 12import torchimport torch.nn as nn PyTorch中储存数据的壳子叫tensor，而NumPy叫做ndarray。在上面的例子中，PyTorch的tensor不仅可以和NumPy中的ndarray相互切换，而且里面的数据类型都是一样的。 TABLE 更重要的是，NumPy中有的对ndarray的各种操作，PyTorch中的tensor也都一摸一样。这时很多同学就会反问了：“这不是基本操作吗？”对，正式因为基本操作，PyTorch就直接拿来一用，这不香吗？虽然，NumPy底层是用C实现的，而PyTorch大多功能是用C++实现的。但是，用什么方式和语言实现都是次要，关键是PyTorch模仿了很多NumPy东西。比如功能设计的非常人性，运算速度快，误差极小，从数据结构到各种算法的优化方式。最最最重要的是ndarray支持并行化运算（向量化运算），而且底层使用C语言编写，内部解除了GIL（全局解释器锁），其对数组的操作速度不受Python解释器的限制。这些NumPy的优点使得PyTorch去实现很多设计理念变的非常容易。 最后大量练习写PyTorch代码！很多东西靠看是学不会的，自动动手写一写。哪怕重写一些教程都比死记硬背的要好的多。 在创业公司工作，每年都会遇到一到两个大型开源项目，时不时还有很多小的开源项目。在FLAG工作也一样，虽然基本都在做公司的东西，但是也需要经常去学习和参考别人是怎么做的，有什么新的设计理念，有什么新的技术，有什么新功能需要实现。所以，希望同学可以掌握或者总结一套自己的学习方法，去面对未来高速发展的社会。 再次感谢这些做开源项目的人！致敬！ 参考和学习资料 Contact Me GitHub LinkedIn Google Scholar Curriculum Vitae 引用"}],"posts":[{"title":"Biography","slug":"biography","date":"2020-07-11T17:52:03.537Z","updated":"2020-07-11T17:52:03.537Z","comments":true,"path":"2020/07/11/biography/","link":"","permalink":"https://weikunhan.github.io/2020/07/11/biography/","excerpt":"","text":"About Weikun Han is currently working as a research assistant for Dr. Fuxin Li on 3D Point Cloud algorithms. Weikun Han was a computer vision scientist at Clobotics where he worked on multiple computer vision tasks (image classification, object detection, semantic segmentation). Beside modeling to many real-life challenging problems, he as machine learning engineer to work on many machine learning system design tasks (ceiling analysis dashboard, active learning pipeline, image retrieval system, distributed data pipeline, online learning pipeline), and he as software development engineer to work on many infrastructure development tasks (product search website, data operations platform). Weikun Han received an M.S. degree in electrical and computer engineering from UCLA, supervised by Prof. Lei He. He received a B.S. degree in electrical engineering from ISU, supervised by Prof. Liang Dong. He also received the IEEE NANO best student paper award, and he co-authored 5 publications in major journals and conferences during the undergraduate research study. Interests Machine Learning Deep Learning Computer Vision Education M.S., Electrical and Computer Engineering, University of California Los Angeles, 2018 B.S., Electrical Engineering, Iowa State University, 2016","categories":[],"tags":[]},{"title":"News","slug":"news","date":"2020-07-01T21:24:27.401Z","updated":"2020-07-01T21:24:27.401Z","comments":true,"path":"2020/07/01/news/","link":"","permalink":"https://weikunhan.github.io/2020/07/01/news/","excerpt":"","text":"News [7/1/2020] I am working as a research assistant for Dr. Fuxin Li on 3D Point Cloud algorithms. Learn More [6/1/2020] I will join Dr. Fuxin Li’s research group as research assistant intern at Oregon State University. Learn More [5/29/2020] I will resign from Clobotics Global. I am actively looking for a research group to keep proceeding with my Ph.D. study. Learn More","categories":[],"tags":[]}],"categories":[],"tags":[]}